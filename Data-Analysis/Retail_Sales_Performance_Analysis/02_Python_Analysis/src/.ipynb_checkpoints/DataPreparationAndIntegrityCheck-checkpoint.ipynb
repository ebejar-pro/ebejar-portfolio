{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a62df5e3-3043-4f97-931d-8a4ef24e7433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded {name}: {len(data_frames[name]):,} rows.\n",
      "✅ Loaded {name}: {len(data_frames[name]):,} rows.\n",
      "✅ Loaded {name}: {len(data_frames[name]):,} rows.\n",
      "✅ Loaded {name}: {len(data_frames[name]):,} rows.\n",
      "✅ Loaded {name}: {len(data_frames[name]):,} rows.\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "\n",
      "--- LOGGING COMPLETE ---\n",
      "All output has been saved to: data_preparation_integrity_log.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys # Added for logging redirection\n",
    "import datetime # Added for timestamping the log\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "FILES = {\n",
    "    \"customers\": DATA_DIR / \"customers.csv\",\n",
    "    \"employees\": DATA_DIR / \"employees.csv\",\n",
    "    \"products\": DATA_DIR / \"products.csv\",\n",
    "    \"sales\": DATA_DIR / \"sales.csv\",\n",
    "    \"sales_commission\": DATA_DIR / \"sales_commission.csv\"\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# Load data\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "customers = pd.read_csv(DATA_DIR / \"customers.csv\")\n",
    "employees = pd.read_csv(DATA_DIR / \"employees.csv\")\n",
    "products = pd.read_csv(DATA_DIR / \"products.csv\")\n",
    "sales = pd.read_csv(DATA_DIR / \"sales.csv\", parse_dates=[\"sale_date\"])\n",
    "sales_commission = pd.read_csv(DATA_DIR / \"sales_commission.csv\", parse_dates=[\"created_at\"])\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def safe_print(message: str, file=sys.stdout):\n",
    "    \"\"\"\n",
    "    Prints a message safely by replacing characters that can't be encoded.\n",
    "    Prevents crashes when emojis or other Unicode symbols are not supported\n",
    "    by the current output encoding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(message, file=file)\n",
    "    except UnicodeEncodeError:\n",
    "        # Replace unsupported characters with '?'\n",
    "        safe_message = message.encode(file.encoding, errors='replace').decode(file.encoding)\n",
    "        print(safe_message, file=file)\n",
    "\n",
    "def load_data(files: dict) -> dict:\n",
    "    \"\"\"Loads all CSV files into a dictionary of pandas DataFrames.\"\"\"\n",
    "    data_frames = {}\n",
    "    print(\"--- 1. Loading DataFrames ---\")\n",
    "    for name, path in files.items():\n",
    "        try:\n",
    "            if name == 'sales':\n",
    "                try:\n",
    "                    data_frames[name] = pd.read_csv(path, parse_dates=['sale_date', 'dispatch_date'])\n",
    "                except Exception:\n",
    "                    data_frames[name] = pd.read_csv(path)\n",
    "                    safe_print(\"⚠️ Warning: Could not parse dates automatically for {name}. Data types need verification.\")\n",
    "            else:\n",
    "                data_frames[name] = pd.read_csv(path)\n",
    "            safe_print(\"✅ Loaded {name}: {len(data_frames[name]):,} rows.\")\n",
    "        except FileNotFoundError:\n",
    "            safe_print(\"❌ ERROR: File not found at path: {path}\")\n",
    "            return None \n",
    "    return data_frames\n",
    "\n",
    "def clean_and_inspect_data(dfs: dict):\n",
    "    \"\"\"Checks for nulls, duplicates, and verifies data types.\"\"\"\n",
    "    print(\"\\n--- 2. Data Cleaning and Inspection ---\")\n",
    "    for name, df in dfs.items():\n",
    "        print(f\"\\n[Table: {name}]\")\n",
    "        \n",
    "        # Check for Nulls\n",
    "        null_count = df.isnull().sum().sum()\n",
    "        if null_count > 0:\n",
    "            safe_print(\"⚠️ Warning: Found {null_count} total null values.\")\n",
    "        else:\n",
    "            safe_print(\"✅ No null values found.\")\n",
    "\n",
    "        # Check for Duplicates\n",
    "        duplicate_count = df.duplicated().sum()\n",
    "        if duplicate_count > 0:\n",
    "            safe_print(\"⚠️ Warning: Found {duplicate_count} duplicate rows. Dropping them.\")\n",
    "            dfs[name] = df.drop_duplicates()\n",
    "        else:\n",
    "            safe_print(\"✅ No exact duplicate rows found.\")\n",
    "        \n",
    "        # Display Data Types\n",
    "        print(\"Data Types:\")\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        # CRITICAL INSPECTION: Print column names\n",
    "        if name == 'products':\n",
    "             print(\"\\n!!! CHECK: PRODUCTS COLUMNS !!!\")\n",
    "             print(df.columns.tolist())\n",
    "        if name == 'employees':\n",
    "             print(\"\\n!!! CHECK: EMPLOYEES COLUMNS !!!\")\n",
    "             print(df.columns.tolist())\n",
    "        if name == 'customers':\n",
    "             print(\"\\n!!! CHECK: CUSTOMERS COLUMNS !!!\")\n",
    "             print(df.columns.tolist())\n",
    "\n",
    "\n",
    "def merge_data(dfs: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges all relevant tables into a single master analysis DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 3. Merging DataFrames (Star Schema Join) ---\")\n",
    "\n",
    "    # 1. Merge Sales (Fact) with Products (Dimensions)\n",
    "    # We rely on your updated CSV having the 'category' column now.\n",
    "    master_df = dfs['sales'].merge(\n",
    "        dfs['products'][['product_id', 'product_name', 'price', 'cost_price', 'category']],\n",
    "        on='product_id',\n",
    "        how='left',\n",
    "        suffixes=('_sales', '_product')\n",
    "    )\n",
    "\n",
    "    # 2. Merge with Employees\n",
    "    # Correctly using 'job_code' instead of 'department'\n",
    "    master_df = master_df.merge(\n",
    "        dfs['employees'][['employee_id', 'name', 'job_code']], \n",
    "        on='employee_id',\n",
    "        how='left'\n",
    "    ).rename(columns={'name': 'employee_name'}) \n",
    "\n",
    "    # 3. Merge with Customers\n",
    "    # FIX: Corrected to use 'name' instead of 'first_name'/'last_name' and removed 'city' \n",
    "    # based on user feedback about the actual CSV structure.\n",
    "    master_df = master_df.merge(\n",
    "        dfs['customers'][['customer_id', 'name', 'age']],\n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    ).rename(columns={'name': 'customer_name'}) # Rename 'name' column for clarity\n",
    "    \n",
    "    print(f\"✅ Final Master DataFrame created with {len(master_df):,} rows.\")\n",
    "    return master_df\n",
    "\n",
    "def feature_engineer_financials(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Creates core financial columns (Revenue, COGS, Profit, Fulfillment Days).\"\"\"\n",
    "    print(\"\\n--- 4. Feature Engineering: Financial Metrics ---\")\n",
    "    \n",
    "    # Calculate Total Revenue for each sale row\n",
    "    df['Revenue'] = df['quantity'] * df['price']\n",
    "    \n",
    "    # Calculate Total Cost of Goods Sold (COGS)\n",
    "    df['COGS'] = df['quantity'] * df['cost_price']\n",
    "    \n",
    "    # Calculate Total Profit (Gross Margin)\n",
    "    df['Profit'] = df['Revenue'] - df['COGS']\n",
    "    \n",
    "    # Calculate the Fulfillment Latency\n",
    "    df['Fulfillment_Days'] = (df['dispatch_date'] - df['sale_date']).dt.days\n",
    "\n",
    "    print(\"✅ New columns created: Revenue, COGS, Profit, Fulfillment_Days.\")\n",
    "    \n",
    "    # Display quick summary of new columns\n",
    "    print(\"\\nQuick Financial Summary:\")\n",
    "    print(f\"Total Revenue: ${df['Revenue'].sum():,.2f}\")\n",
    "    print(f\"Total Profit: ${df['Profit'].sum():,.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- 5. EXECUTION BLOCK (Run this cell last) ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Define the log file path\n",
    "    log_file_path = \"data_preparation_integrity_log.txt\"\n",
    "    original_stdout = sys.stdout # Save the original stdout\n",
    "\n",
    "    try:\n",
    "        # 1. Redirect all print output to the log file\n",
    "        with open(log_file_path, 'w', encoding='utf-8') as f:\n",
    "            sys.stdout = f  # Change the standard output to the file stream \n",
    "            print(f\"--- RETAIL ANALYSIS LOG STARTED: {datetime.datetime.now()} ---\")\n",
    "            \n",
    "            # Load all data\n",
    "            dfs = load_data(FILES)\n",
    "\n",
    "            if dfs:\n",
    "                # Inspect and clean \n",
    "                clean_and_inspect_data(dfs)\n",
    "\n",
    "                # Merge into a single analysis table\n",
    "                master_df = merge_data(dfs)\n",
    "\n",
    "                # Create financial calculation columns\n",
    "                final_df = feature_engineer_financials(master_df)\n",
    "\n",
    "                # Display the first few rows of the final dataset\n",
    "                print(\"\\n--- 5. Final Master Data Preview (First 5 Rows) ---\")\n",
    "                # Ensure the DataFrame is converted to string for logging\n",
    "                print(final_df[['sale_id', 'sale_date', 'customer_name', 'age', 'employee_name', 'job_code', 'product_name', 'category', 'Revenue']].head().to_string())\n",
    "            \n",
    "            print(f\"\\n--- RETAIL ANALYSIS LOG FINISHED: {datetime.datetime.now()} ---\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Print errors to the console (original stdout) even if logging failed\n",
    "        sys.stdout = original_stdout\n",
    "        print(f\"An error occurred during execution: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # 2. Reset stdout to the original value (the Jupyter cell output)\n",
    "        sys.stdout = original_stdout\n",
    "        print(f\"\\n--- LOGGING COMPLETE ---\")\n",
    "        print(f\"All output has been saved to: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e2374-8e45-4473-87a2-63b282eb6285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
