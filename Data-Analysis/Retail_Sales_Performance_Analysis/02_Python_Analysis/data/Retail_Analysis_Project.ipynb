{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76f73ae-0116-4235-8e93-94df134539c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading DataFrames ---\n",
      "✅ Loaded sales: 201 rows.\n",
      "✅ Loaded customers: 47 rows.\n",
      "✅ Loaded employees: 11 rows.\n",
      "✅ Loaded products: 13 rows.\n",
      "✅ Loaded sales_commission: 197 rows.\n",
      "\n",
      "--- 2. Data Cleaning and Inspection ---\n",
      "\n",
      "[Table: sales]\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "Data Types:\n",
      "sale_id                   int64\n",
      "customer_id               int64\n",
      "product_id                int64\n",
      "sale_date        datetime64[ns]\n",
      "quantity                  int64\n",
      "employee_id               int64\n",
      "supplier_id               int64\n",
      "sales_channel            object\n",
      "dispatch_date    datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "[Table: customers]\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "Data Types:\n",
      "customer_id     int64\n",
      "name           object\n",
      "email          object\n",
      "age             int64\n",
      "dtype: object\n",
      "\n",
      "[Table: employees]\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "Data Types:\n",
      "employee_id      int64\n",
      "name            object\n",
      "salary         float64\n",
      "job_code        object\n",
      "commission     float64\n",
      "dtype: object\n",
      "\n",
      "[Table: products]\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "Data Types:\n",
      "product_id        int64\n",
      "product_name     object\n",
      "price           float64\n",
      "supplier_id       int64\n",
      "cost_price      float64\n",
      "stock_level       int64\n",
      "dtype: object\n",
      "\n",
      "[Table: sales_commission]\n",
      "✅ No null values found.\n",
      "✅ No exact duplicate rows found.\n",
      "Data Types:\n",
      "commission_id          int64\n",
      "employee_id            int64\n",
      "sale_id                int64\n",
      "commission_amount    float64\n",
      "created_at            object\n",
      "dtype: object\n",
      "\n",
      "--- 3. Merging DataFrames (Star Schema Join) ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['category'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 134\u001b[0m\n\u001b[0;32m    131\u001b[0m clean_and_inspect_data(dfs)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Merge into a single analysis table\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m master_df \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Create financial calculation columns\u001b[39;00m\n\u001b[0;32m    137\u001b[0m final_df \u001b[38;5;241m=\u001b[39m feature_engineer_financials(master_df)\n",
      "Cell \u001b[1;32mIn[1], line 68\u001b[0m, in \u001b[0;36mmerge_data\u001b[1;34m(dfs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- 3. Merging DataFrames (Star Schema Join) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# 1. Merge Sales (Fact) with Products and Employees (Dimensions)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# We start with sales as the central fact table\u001b[39;00m\n\u001b[0;32m     67\u001b[0m master_df \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m---> 68\u001b[0m     \u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproducts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcost_price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     69\u001b[0m     on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     70\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     71\u001b[0m     suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_sales\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_product\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# 2. Merge with Employees\u001b[39;00m\n\u001b[0;32m     75\u001b[0m master_df \u001b[38;5;241m=\u001b[39m master_df\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m     76\u001b[0m     dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployees\u001b[39m\u001b[38;5;124m'\u001b[39m][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployee_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m     77\u001b[0m     on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployee_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     78\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     79\u001b[0m )\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployee_name\u001b[39m\u001b[38;5;124m'\u001b[39m}) \u001b[38;5;66;03m# Rename 'name' to avoid confusion\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaRetailSales\\lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4118\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4119\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4121\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaRetailSales\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaRetailSales\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6263\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['category'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Define the file paths for your five CSV files.\n",
    "# ASSUMPTION: These files are in the same directory as this script.\n",
    "FILES = {\n",
    "    'sales': 'sales.csv',\n",
    "    'customers': 'customers.csv',\n",
    "    'employees': 'employees.csv',\n",
    "    'products': 'products.csv',\n",
    "    'sales_commission': 'sales_commission.csv'\n",
    "}\n",
    "\n",
    "def load_data(files: dict) -> dict:\n",
    "    \"\"\"Loads all CSV files into a dictionary of pandas DataFrames.\"\"\"\n",
    "    data_frames = {}\n",
    "    print(\"--- 1. Loading DataFrames ---\")\n",
    "    for name, path in files.items():\n",
    "        try:\n",
    "            # We assume sales_date and dispatch_date are present in sales.csv\n",
    "            if name == 'sales':\n",
    "                data_frames[name] = pd.read_csv(path, parse_dates=['sale_date', 'dispatch_date'])\n",
    "            else:\n",
    "                data_frames[name] = pd.read_csv(path)\n",
    "            print(f\"✅ Loaded {name}: {len(data_frames[name]):,} rows.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ ERROR: File not found at path: {path}\")\n",
    "            return None # Exit if critical file is missing\n",
    "    return data_frames\n",
    "\n",
    "def clean_and_inspect_data(dfs: dict):\n",
    "    \"\"\"Checks for nulls, duplicates, and verifies data types.\"\"\"\n",
    "    print(\"\\n--- 2. Data Cleaning and Inspection ---\")\n",
    "    for name, df in dfs.items():\n",
    "        print(f\"\\n[Table: {name}]\")\n",
    "        \n",
    "        # Check for Nulls\n",
    "        null_count = df.isnull().sum().sum()\n",
    "        if null_count > 0:\n",
    "            print(f\"⚠️ Warning: Found {null_count} total null values.\")\n",
    "            # Optional: Impute or drop nulls here if necessary\n",
    "        else:\n",
    "            print(\"✅ No null values found.\")\n",
    "\n",
    "        # Check for Duplicates (based on all columns)\n",
    "        duplicate_count = df.duplicated().sum()\n",
    "        if duplicate_count > 0:\n",
    "            print(f\"⚠️ Warning: Found {duplicate_count} duplicate rows. Dropping them.\")\n",
    "            dfs[name] = df.drop_duplicates()\n",
    "        else:\n",
    "            print(\"✅ No exact duplicate rows found.\")\n",
    "        \n",
    "        # Display Data Types\n",
    "        print(\"Data Types:\")\n",
    "        print(df.dtypes)\n",
    "\n",
    "def merge_data(dfs: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges all relevant tables into a single master analysis DataFrame.\n",
    "    Uses the Star Schema joins (Fact table `sales` to Dimension tables).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 3. Merging DataFrames (Star Schema Join) ---\")\n",
    "\n",
    "    # 1. Merge Sales (Fact) with Products and Employees (Dimensions)\n",
    "    # We start with sales as the central fact table\n",
    "    master_df = dfs['sales'].merge(\n",
    "        dfs['products'][['product_id', 'product_name', 'price', 'cost_price', 'category']],\n",
    "        on='product_id',\n",
    "        how='left',\n",
    "        suffixes=('_sales', '_product')\n",
    "    )\n",
    "    \n",
    "    # 2. Merge with Employees\n",
    "    master_df = master_df.merge(\n",
    "        dfs['employees'][['employee_id', 'name', 'department']],\n",
    "        on='employee_id',\n",
    "        how='left'\n",
    "    ).rename(columns={'name': 'employee_name'}) # Rename 'name' to avoid confusion\n",
    "\n",
    "    # 3. Merge with Customers\n",
    "    master_df = master_df.merge(\n",
    "        dfs['customers'][['customer_id', 'first_name', 'last_name', 'age', 'city']],\n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    ).rename(columns={'first_name': 'customer_first_name', 'last_name': 'customer_last_name'})\n",
    "\n",
    "    # 4. Optional: Merge Sales Commission (Use this later for validation)\n",
    "    # For now, we keep the main analysis DF focused on the core transaction details\n",
    "    \n",
    "    print(f\"✅ Final Master DataFrame created with {len(master_df):,} rows.\")\n",
    "    print(\"Columns in Master DF (Ready for calculation):\")\n",
    "    print(master_df.columns.tolist())\n",
    "    return master_df\n",
    "\n",
    "def feature_engineer_financials(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Creates core financial columns (Revenue, COGS, Profit) using NumPy/Pandas vectorization.\"\"\"\n",
    "    print(\"\\n--- 4. Feature Engineering: Financial Metrics ---\")\n",
    "    \n",
    "    # Calculate Total Revenue for each sale row\n",
    "    # Price is the selling price\n",
    "    df['Revenue'] = df['quantity'] * df['price']\n",
    "    \n",
    "    # Calculate Total Cost of Goods Sold (COGS)\n",
    "    # cost_price is the cost to the retailer\n",
    "    df['COGS'] = df['quantity'] * df['cost_price']\n",
    "    \n",
    "    # Calculate Total Profit (Gross Margin)\n",
    "    df['Profit'] = df['Revenue'] - df['COGS']\n",
    "    \n",
    "    # Calculate the Fulfillment Latency (same as your calculated column in Power BI)\n",
    "    df['Fulfillment_Days'] = (df['dispatch_date'] - df['sale_date']).dt.days\n",
    "\n",
    "    print(\"✅ New columns created: Revenue, COGS, Profit, Fulfillment_Days.\")\n",
    "    \n",
    "    # Display quick summary of new columns\n",
    "    print(\"\\nQuick Financial Summary:\")\n",
    "    print(f\"Total Revenue: ${df['Revenue'].sum():,.2f}\")\n",
    "    print(f\"Total Profit: ${df['Profit'].sum():,.2f}\")\n",
    "    print(f\"Avg Profit per Sale: ${df['Profit'].mean():,.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- 5. MAIN EXECUTION BLOCK ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load all data\n",
    "    dfs = load_data(FILES)\n",
    "\n",
    "    if dfs:\n",
    "        # Inspect and clean (drop duplicates)\n",
    "        clean_and_inspect_data(dfs)\n",
    "\n",
    "        # Merge into a single analysis table\n",
    "        master_df = merge_data(dfs)\n",
    "\n",
    "        # Create financial calculation columns\n",
    "        final_df = feature_engineer_financials(master_df)\n",
    "\n",
    "        # The final_df is now ready for Scenario 2 & 3 Analysis!\n",
    "        \n",
    "        # Display the first few rows of the final dataset\n",
    "        print(\"\\n--- 5. Final Master Data Preview ---\")\n",
    "        print(final_df[['sale_id', 'sale_date', 'employee_name', 'product_name', 'Revenue', 'Profit', 'Fulfillment_Days']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75636872-f59f-473b-85a6-9c520c154e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0589c3-4669-43c5-823f-783d5aa8dec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
